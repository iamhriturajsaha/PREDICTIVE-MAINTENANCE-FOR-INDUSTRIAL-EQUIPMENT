# -*- coding: utf-8 -*-
"""Industrial.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yycuorZdATTnA7vPEJ14pzNYmH-pJ-Uc
"""

# Step 1 - Import Required Libraries

!pip install pandas numpy scikit-learn matplotlib seaborn plotly streamlit fastapi uvicorn joblib --quiet
import os
import zipfile
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import joblib

# Step 3 - Upload & Extract Dataset

from google.colab import files
print("üìÇ Please upload Dataset.zip")
uploaded = files.upload()
zip_path = list(uploaded.keys())[0]
extract_path = "/content/NASA_Dataset"
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)
print("‚úÖ Dataset extracted successfully!")

# Step 3 - Flexible Loader for NASA Dataset

def load_nasa_dataset(extract_path, fd="FD001"):
    """
    Load NASA CMAPSS dataset for a given FD (FD001‚ÄìFD004).
    """
    train_path = os.path.join(extract_path, f"Dataset/Train/Train {fd}.csv")
    test_path  = os.path.join(extract_path, f"Dataset/Test/Test {fd}.csv")
    rul_path   = os.path.join(extract_path, f"Dataset/RUL/RUL {fd}.csv")
    # Column names
    cols = ["unit_number","time_in_cycles"] + \
           [f"operational_setting_{i}" for i in range(1,4)] + \
           [f"sensor_measurement_{i}" for i in range(1,22)]
    # Load train
    train_df = pd.read_csv(train_path, sep=" ", header=None)
    train_df = train_df.dropna(axis=1, how='all')
    train_df.columns = cols
    # Load test
    test_df = pd.read_csv(test_path, sep=" ", header=None)
    test_df = test_df.dropna(axis=1, how='all')
    test_df.columns = cols
    # Load RUL
    rul_df = pd.read_csv(rul_path, header=None)
    print(f"‚úÖ {fd} Loaded Successfully!")
    print("Train shape:", train_df.shape)
    print("Test shape:", test_df.shape)
    print("RUL shape:", rul_df.shape)
    return train_df, test_df, rul_df
# Example - Load FD001
train_df, test_df, rul_df = load_nasa_dataset(extract_path, fd="FD001")
train_df.head(50)

# Step 4 - Preprocessing & Feature Engineering

def preprocess_data(train_df):
    # Compute RUL (Remaining Useful Life)
    rul_train = train_df.groupby('unit_number')['time_in_cycles'].max().reset_index()
    rul_train.columns = ['unit_number','max_cycle']
    train_df = train_df.merge(rul_train, on='unit_number', how='left')
    train_df['RUL'] = train_df['max_cycle'] - train_df['time_in_cycles']
    train_df.drop(columns=['max_cycle'], inplace=True)
    # Select features (sensors + operational settings)
    features = [col for col in train_df.columns if "sensor" in col or "operational" in col]
    # Normalize
    scaler = StandardScaler()
    train_df[features] = scaler.fit_transform(train_df[features])
    return train_df, features, scaler
train_df, features, scaler = preprocess_data(train_df)
print("‚úÖ Preprocessing done. Features count:", len(features))
train_df.head(50)

# Step 5 - LSTM Model Training

!pip install tensorflow --quiet
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input   # ‚úÖ Added Input here
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.preprocessing import MinMaxScaler
import matplotlib.pyplot as plt
import numpy as np
import joblib
# Scale features
scaler = MinMaxScaler()
train_df[features] = scaler.fit_transform(train_df[features])
# Create sequences per engine
def create_sequences(df, features, seq_length=30):
    X, y = [], []
    for engine_id in df['unit_number'].unique():
        engine_data = df[df['unit_number'] == engine_id].sort_values('time_in_cycles')
        sensor_data = engine_data[features].values
        rul_data = engine_data['RUL'].values
        for i in range(len(sensor_data) - seq_length):
            X.append(sensor_data[i:i+seq_length])
            y.append(rul_data[i+seq_length])
    return np.array(X), np.array(y)
SEQ_LEN = 30
X, y = create_sequences(train_df, features, seq_length=SEQ_LEN)
# Train/Validation split
split = int(0.8 * len(X))
X_train, X_val = X[:split], X[split:]
y_train, y_val = y[:split], y[split:]
print("X_train shape:", X_train.shape)
print("X_val shape:", X_val.shape)
# Build Advanced LSTM Model
model = Sequential([
    Input(shape=(SEQ_LEN, len(features))),
    LSTM(128, activation='tanh', return_sequences=True),
    Dropout(0.3),
    LSTM(64, activation='tanh'),
    Dropout(0.3),
    Dense(32, activation='relu'),
    Dense(1, activation='linear')
])
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
model.summary()
# Callbacks
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
checkpoint = ModelCheckpoint("best_lstm_model.keras", monitor='val_loss', save_best_only=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)
# Train model
history = model.fit(
    X_train, y_train,
    validation_data=(X_val, y_val),
    epochs=50,
    batch_size=64,
    callbacks=[early_stopping, checkpoint, reduce_lr],
    verbose=1
)
# Evaluate
val_loss, val_mae = model.evaluate(X_val, y_val, verbose=0)
print(f"üìä Advanced Model Performance:\nMAE: {val_mae:.2f}\nRMSE: {np.sqrt(val_loss):.2f}")
# Plot training curves
plt.figure(figsize=(12,5))
# Loss curve
plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel("Epochs")
plt.ylabel("MSE Loss")
plt.title("Training vs Validation Loss")
plt.legend()
# MAE curve
plt.subplot(1,2,2)
plt.plot(history.history['mae'], label='Train MAE')
plt.plot(history.history['val_mae'], label='Validation MAE')
plt.xlabel("Epochs")
plt.ylabel("MAE")
plt.title("Training vs Validation MAE")
plt.legend()
plt.show()
# Save Model & Scaler
model.save("final_predictive_maintenance_lstm.keras")
joblib.dump(scaler, "scaler.pkl")

# Step 6 - Real-time Monitoring & Alerting System

import numpy as np
import pandas as pd
import joblib
import tensorflow as tf
import time
# Load saved model and scaler
model = tf.keras.models.load_model("final_predictive_maintenance_lstm.keras")
scaler = joblib.load("scaler.pkl")
# Real-time monitoring function
def simulate_real_time_monitoring(test_df, features, seq_len=30, threshold=20, last_n=20):
    engines = test_df['unit_number'].unique()
    for engine_id in engines:
        engine_data = test_df[test_df['unit_number'] == engine_id].sort_values('time_in_cycles')
        # Scale using DataFrame to preserve feature names -> fixes warning
        sensor_data_scaled = pd.DataFrame(
            scaler.transform(engine_data[features]),
            columns=features
        ).values
        # Monitor only last N cycles
        start_idx = max(seq_len, len(sensor_data_scaled) - last_n)
        print(f"\nüîç Monitoring Engine {engine_id} (last {last_n} cycles)...")
        for i in range(start_idx, len(sensor_data_scaled)):
            X_live = np.expand_dims(sensor_data_scaled[i-seq_len:i], axis=0)
            predicted_rul = model.predict(X_live, verbose=0)[0][0]
            if predicted_rul < threshold:
                print(f"üö® ALERT | Engine {engine_id} | Cycle {engine_data.iloc[i]['time_in_cycles']} "
                      f"| Predicted RUL: {predicted_rul:.2f}")
            # Simulate streaming delay (optional)
            time.sleep(0.3)
# Example usage
simulate_real_time_monitoring(test_df, features, seq_len=30, threshold=25, last_n=20)

# Step 7 - Streamlit App Creation

app_code = """
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import tensorflow as tf
import plotly.graph_objects as go

# ----------------- LOAD MODEL & SCALER ----------------- #
model = tf.keras.models.load_model("final_predictive_maintenance_lstm.keras", compile=False, safe_mode=False)
scaler = joblib.load("scaler.pkl")

# Features used in training (must match exactly!)
features = [
    "operational_setting_1",
    "operational_setting_2",
    "operational_setting_3"
] + [f"sensor_measurement_{i}" for i in range(1, 22)]   # 21 sensors

seq_len = 30

# ----------------- SAFE PREDICT WRAPPER ----------------- #
def safe_predict(model, X):
    X = tf.convert_to_tensor(X, dtype=tf.float32)
    preds = model(X, training=False)   # forward pass only
    return preds.numpy()

# ----------------- STREAMLIT LAYOUT ----------------- #
st.set_page_config(page_title="‚öôÔ∏è Equipment Health Dashboard", layout="wide")

st.title("‚öôÔ∏è Equipment Health Monitoring Dashboard")
st.markdown("A real-time predictive maintenance dashboard powered by **LSTM Deep Learning**.")

# Sidebar settings
st.sidebar.header("‚öôÔ∏è Controls")
threshold = st.sidebar.slider("Failure Alert Threshold (RUL)", 5, 50, 20)
last_n = st.sidebar.slider("Show Last N Cycles", 10, 100, 30)
engine_ids = st.sidebar.multiselect("Select Engines to Monitor", [1, 2, 3, 4, 5], default=[1])

# Auto-refresh every 5 seconds
try:
    from streamlit_autorefresh import st_autorefresh
except ImportError:
    def st_autorefresh(*args, **kwargs):
        return 0
count = st_autorefresh(interval=5000, key="refresh_counter")

# ----------------- SIMULATED TEST DATA ----------------- #
def generate_engine_data(engine_id, n_cycles=200):
    np.random.seed(engine_id)
    cycles = np.arange(1, n_cycles + 1)

    df = pd.DataFrame({
        "unit_number": engine_id,
        "time_in_cycles": cycles,
        "operational_setting_1": np.random.uniform(-1, 1, n_cycles),
        "operational_setting_2": np.random.uniform(0, 1, n_cycles),
        "operational_setting_3": np.random.uniform(0, 1, n_cycles),
    })

    # Add 21 sensor signals
    for i in range(1, 22):
        df[f"sensor_measurement_{i}"] = np.sin(cycles / (10+i)) + np.random.normal(0, 0.1, n_cycles)

    # Scale features
    df[features] = scaler.transform(df[features])
    return df

# ----------------- GLOBAL ALERT TRACKING ----------------- #
global_status = "‚úÖ All Engines Healthy"
global_color = "green"
engine_status = {}

for engine_id in engine_ids:
    df = generate_engine_data(engine_id)
    max_cycle = min(seq_len + count, len(df))
    stream_df = df.iloc[:max_cycle]

    preds, rul_cycles = [], []
    for i in range(seq_len, len(stream_df)):
        X_live = np.expand_dims(stream_df[features].iloc[i-seq_len:i].values, axis=0)
        predicted_rul = max(0, safe_predict(model, X_live)[0][0])  # <-- FIXED
        preds.append(predicted_rul)
        rul_cycles.append(stream_df.iloc[i]["time_in_cycles"])

    if len(preds) > 0:
        latest_rul = preds[-1]
        status = "‚úÖ Healthy"
        color = "green"
        if latest_rul < threshold/2:
            status, color = "üî¥ Critical", "red"
        elif latest_rul < threshold:
            status, color = "üü† Warning", "orange"

        engine_status[engine_id] = (status, color, rul_cycles, preds)

        # Update global status
        if color == "red":
            global_status, global_color = "üö® CRITICAL ALERT: Immediate Maintenance Required!", "red"
        elif color == "orange" and global_color != "red":
            global_status, global_color = "‚ö†Ô∏è Warning: Some Engines Need Attention", "orange"

# ----------------- DISPLAY GLOBAL ALERT BANNER ----------------- #
st.markdown(f\"\"\"
<div style='padding:15px; border-radius:10px; background-color:{global_color};
color:white; font-size:20px; font-weight:bold; text-align:center;'>
{global_status}
</div>
\"\"\", unsafe_allow_html=True)

# ----------------- PER-ENGINE VISUALIZATION ----------------- #
for engine_id, (status, color, rul_cycles, preds) in engine_status.items():
    st.markdown(f"### Engine {engine_id} Status: <span style='color:{color}; font-weight:bold;'>{status}</span>", unsafe_allow_html=True)

    fig = go.Figure()
    fig.add_trace(go.Scatter(
        x=rul_cycles[-last_n:],
        y=preds[-last_n:],
        mode="lines+markers",
        name="Predicted RUL",
        line=dict(color="blue", width=3),
        marker=dict(size=8)
    ))
    fig.add_hline(y=threshold, line_dash="dash", line_color="red", annotation_text="Threshold", annotation_position="top right")
    fig.update_layout(
        title=f"üìâ Predicted Remaining Useful Life (Engine {engine_id})",
        xaxis_title="Cycle",
        yaxis_title="Predicted RUL",
        template="plotly_white",
        height=400,
        margin=dict(l=20, r=20, t=60, b=20),
    )
    st.plotly_chart(fig, use_container_width=True)

    st.subheader(f"Recent Predictions (Engine {engine_id})")
    recent_df = pd.DataFrame({
        "Cycle": rul_cycles[-last_n:],
        "Predicted_RUL": preds[-last_n:]
    })
    st.dataframe(recent_df, use_container_width=True)
"""
# Save to file
with open("app.py", "w") as f:
    f.write(app_code)

# Step 8 - Streamlit App Deployment

# Install necessary packages
!pip install -q streamlit pyngrok streamlit-autorefresh
# Import required libraries
import os
import time
from pyngrok import ngrok, conf
# Configure Ngrok Authentication
NGROK_AUTH_TOKEN = "2z0Oqv0tD166fELGCHwV2gLZwq1_2G2zUQRSs6C27k9vdzxwq"
conf.get_default().auth_token = NGROK_AUTH_TOKEN
# Create logs directory
LOG_DIR = "/content/logs"
os.makedirs(LOG_DIR, exist_ok=True)
# Run Streamlit app in background
!streamlit run app.py --server.port 8501 --server.address 0.0.0.0 > {LOG_DIR}/app_log.txt 2>&1 &
# Give Streamlit a few seconds to start
time.sleep(7)
# Connect Ngrok to the running Streamlit app
public_url = ngrok.connect(8501, "http")
print("üöÄ Your Streamlit app is live at:", public_url.public_url)